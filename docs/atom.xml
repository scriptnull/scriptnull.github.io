<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vishnu Bharathi</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://vishnubharathi.codes/"/>
  <updated>2021-02-03T02:55:21.208Z</updated>
  <id>https://vishnubharathi.codes/</id>
  
  <author>
    <name>Vishnu Bharathi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Speeding up git clone</title>
    <link href="https://vishnubharathi.codes/blog/speeding-up-git-clone/"/>
    <id>https://vishnubharathi.codes/blog/speeding-up-git-clone/</id>
    <published>2021-02-03T02:23:04.000Z</published>
    <updated>2021-02-03T02:55:21.208Z</updated>
    
    <content type="html"><![CDATA[<p>Imagine that you need to take the latest source code of a project and deploy it somewhere by creating some artifacts with it. If it is a git project with a few commits, a simple <code>git clone .....</code> would do it. But if it is a project with thousands of commits, then you might be bored of your time waiting for the git clone to complete while deploying.</p><p>I recently discovered about something called shallow clones in git. The idea is simple: instead of getting all the commits of a git repo, shallow clone only gets the latest commit. The <code>--depth=N</code> flag seems to enable shallow cloning in git where N is the number of latest commits to be fetched.</p><p>Here is an example for comparing how these much of a time you can save:</p><p>I tried cloning the redis project - <a href="https://github.com/redis/redis">https://github.com/redis/redis</a> which had 10,009 commits while cloning.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ time git clone git@github.com:redis&#x2F;redis.git</span><br><span class="line">Cloning into &#39;redis&#39;...</span><br><span class="line">remote: Enumerating objects: 29, done.</span><br><span class="line">remote: Counting objects: 100% (29&#x2F;29), done.</span><br><span class="line">remote: Compressing objects: 100% (21&#x2F;21), done.</span><br><span class="line">remote: Total 71995 (delta 11), reused 17 (delta 8), pack-reused 71966</span><br><span class="line">Receiving objects: 100% (71995&#x2F;71995), 91.10 MiB | 375.00 KiB&#x2F;s, done.</span><br><span class="line">Resolving deltas: 100% (51230&#x2F;51230), done.</span><br><span class="line"></span><br><span class="line">real4m15.938s</span><br><span class="line">user0m13.796s</span><br><span class="line">sys0m2.875s</span><br></pre></td></tr></table></figure><p>So normal clone took <code>4 minutes and 15 seconds</code> to complete.</p><p>Here is the same thing but with a shallow clone<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ time git clone --depth&#x3D;1 git@github.com:redis&#x2F;redis.git</span><br><span class="line">Cloning into &#39;redis&#39;...</span><br><span class="line">remote: Enumerating objects: 937, done.</span><br><span class="line">remote: Counting objects: 100% (937&#x2F;937), done.</span><br><span class="line">remote: Compressing objects: 100% (843&#x2F;843), done.</span><br><span class="line">remote: Total 937 (delta 105), reused 568 (delta 70), pack-reused 0</span><br><span class="line">Receiving objects: 100% (937&#x2F;937), 2.58 MiB | 390.00 KiB&#x2F;s, done.</span><br><span class="line">Resolving deltas: 100% (105&#x2F;105), done.</span><br><span class="line"></span><br><span class="line">real0m12.802s</span><br><span class="line">user0m0.621s</span><br><span class="line">sys0m0.167s</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>And shallow clone took just <code>12 seconds</code> to complete.</p><p>so fast!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Imagine that you need to take the latest source code of a project and deploy it somewhere by creating some artifacts with it. If it is a 
      
    
    </summary>
    
    
      <category term="devops" scheme="https://vishnubharathi.codes/tags/devops/"/>
    
      <category term="productivity" scheme="https://vishnubharathi.codes/tags/productivity/"/>
    
      <category term="tools" scheme="https://vishnubharathi.codes/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>My blub study syllabus</title>
    <link href="https://vishnubharathi.codes/blog/my-blub-study-syllabus/"/>
    <id>https://vishnubharathi.codes/blog/my-blub-study-syllabus/</id>
    <published>2021-01-22T14:15:45.000Z</published>
    <updated>2021-02-03T02:55:21.208Z</updated>
    
    <content type="html"><![CDATA[<p>Few days back, I happened to read this blog post <a href="https://www.benkuhn.net/blub/">“In defense of blub studies”</a> by <a href="https://www.benkuhn.net/">Benkuhn</a> (one of my favorite bloggers). It introduced me to a new term called “blub studies”. In case if you are not aware of it, I am going to copy paste a few lines from Ben’s post here.</p><blockquote><p>Blub studies is a never-ending treadmill of engineering know-how. It’s the fiddly technical details of how Git stores data, or how Postgres locking semantics caused your migration to bring down prod, or why pip install failed this time. It’s what goes on inside the boiler rooms of your computer.</p></blockquote><p>To put it simply - it is usually the boring stuff that you get away by googling and arriving at stack overflow. The idea is to learn things that help in day to day task instead of chasing the shiny new thing that got released yesterday on HN. we have underrated blub studies for a long time. I find it more satisfying to learn about them these days because they improve my workflow and speed in getting stuff done.</p><p>So, I am listing down my blub study syllabus below.</p><h2 id="Utils"><a href="#Utils" class="headerlink" title="Utils"></a>Utils</h2><ul><li style="list-style: none"><input type="checkbox"></input> <a href="https://www-users.york.ac.uk/~mijp1/teaching/2nd_year_Comp_Lab/guides/grep_awk_sed.pdf">grep, awk and sed – three VERY useful command-line utilities</a></li><li style="list-style: none"><input type="checkbox"></input> <a href="https://www.gnu.org/software/sed/manual/sed.html">sed, a stream editor</a> - GNU software manual</li><li style="list-style: none"><input type="checkbox"></input> <a href="https://www.gnu.org/software/make/manual/make.html">GNU make</a> - GNU software manual</li></ul><h2 id="Go"><a href="#Go" class="headerlink" title="Go"></a>Go</h2><ul><li style="list-style: none"><input type="checkbox"></input> <a href="https://golang.org/doc/modules/managing-dependencies">Managing dependencies</a></li><li style="list-style: none"><input type="checkbox"></input> <a href="https://golang.org/doc/modules/developing">Developing and publishing modules</a></li></ul><p>(To be continued….)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Few days back, I happened to read this blog post &lt;a href=&quot;https://www.benkuhn.net/blub/&quot;&gt;“In defense of blub studies”&lt;/a&gt; by &lt;a href=&quot;htt
      
    
    </summary>
    
    
      <category term="programming" scheme="https://vishnubharathi.codes/tags/programming/"/>
    
      <category term="tools" scheme="https://vishnubharathi.codes/tags/tools/"/>
    
      <category term="reading" scheme="https://vishnubharathi.codes/tags/reading/"/>
    
      <category term="wishlist" scheme="https://vishnubharathi.codes/tags/wishlist/"/>
    
  </entry>
  
  <entry>
    <title>Learning to SWIM</title>
    <link href="https://vishnubharathi.codes/blog/learning-to-swim/"/>
    <id>https://vishnubharathi.codes/blog/learning-to-swim/</id>
    <published>2021-01-03T03:52:20.000Z</published>
    <updated>2021-02-03T02:55:21.204Z</updated>
    
    <content type="html"><![CDATA[<p>I finished reading a research paper called “SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol” last night. This post is going to be about it. If you are here to read my experience about real-world swimming, I have disappointed you - I still don’t know to swim in waters, haha.</p><p>First, I started by reading the Raft paper and was trying to go through a few open-source RAft implementation libraries and see how those libraries are used in software that is currently used in production. The exploration included the following places:</p><ul><li><a href="https://github.com/hashicorp/raft">https://github.com/hashicorp/raft</a></li><li><a href="https://github.com/hashicorp/consul">https://github.com/hashicorp/consul</a></li><li><a href="https://github.com/hashicorp/nomad">https://github.com/hashicorp/nomad</a></li><li><a href="https://github.com/hashicorp/raft-mdb">https://github.com/hashicorp/raft-mdb</a></li></ul><p>Halfway through my Raft journey, I understood that Raft is a consensus algorithm - It is about making a few machines (a cluster) agree on something. But it does not deal with how machines could be added/removed in the cluster. So we need to handle cluster membership outside it. So I started digging the above libraries, I arrived at how cluster membership is done in some of the practical systems (mostly the Hashicorp stack).</p><p>Hashicorp seems to have <a href="https://github.com/hashicorp/serf">Serf</a> which could be used for performing cluster membership. More specifically it uses this <a href="https://github.com/hashicorp/memberlist">memberlist</a> library to do it which is based on this awesome research paper.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A. Das, I. Gupta and A. Motivala, &quot;SWIM: scalable weakly-consistent infection-style process group membership protocol,&quot; Proceedings International Conference on Dependable Systems and Networks, Washington, DC, USA, 2002, pp. 303-312, doi: 10.1109&#x2F;DSN.2002.1028914.</span><br></pre></td></tr></table></figure><p>Here are some of my notes on the SWIM paper. The SWIM paper starts with this nice quote:</p><blockquote><p>As you swim lazily through the milieu,</p><p>The secrets of the world will infect you.</p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The main motivation behind the SWIM paper is</p><blockquote><p>The SWIM effort is motivated by the unscalability of traditional heart-beating protocols, which either impose network loads that grow quadratically with group size, or compromise response times or false positive frequency w.r.t. detecting process crashes.</p></blockquote><p>Before SWIM, the norm is to use a simple many-to-many heart-beating protocol to propagate cluster membership in the cluster. Consider this example:</p><p>If the cluster has n nodes, then it needs to send a message to all the other (n-1) nodes. Hence we end up sending n*(n-1) messages across the network when we want to exchange information about cluster membership. This means that our network load (number of communications that happen on the cluster network) grows quadratically (n<sup>2</sup>) for a group of n nodes in the cluster.</p><p>Example of what we are dealing with here:</p><p>When there are 5 nodes, we will end up making 5<sup>2</sup> = 25 network communications. For 10, it will be 100 and for 100, it will be 10,000. Remember this is just for sharing the knowledge of what nodes are in the cluster. That’s too much and impractical.</p><p>SWIM avoids this quadratic growth.</p><h2 id="Performance-metrics"><a href="#Performance-metrics" class="headerlink" title="Performance metrics"></a>Performance metrics</h2><p>If we are designing a membership system for a cluster, the following performance metrics could be considered:</p><ul><li>Membership propagation speed (should be high)</li><li>Message load on the network (should be low)</li><li>Computation load on the process (should be low)</li><li>False failure detections (should be low)</li></ul><p>Typically we might want to have this in our monitoring system to figure out if something is going wrong with our membership system.</p><p>SWIM tries to give the following performance metrics:</p><blockquote><p>(1) imposes a constant message load per group member;</p><p>(2) detects a process failure in an (expected) constant time at some non-faulty process in the group;</p><p>(3) provides a deterministic bound (as a function of group size) on the local time that a non-faulty process takes to detect failure of another process;</p><p>(4) propagates membership updates, including information about failures, in infection-style (also gossip-style or epidemic-style [2, 8]); the dissemination latency in the group grows slowly (logarithmically) with the number of members;</p><p>(5) provides a mechanism to reduce the rate of false positives by “suspecting” a process before “declaring” it as failed within the group.</p></blockquote><h2 id="Swimming"><a href="#Swimming" class="headerlink" title="Swimming"></a>Swimming</h2><p>I started describing the internal workings of SWIM here to get a deep understanding, but I kind of thought it to be a long-running note that might be lacking good diagrams and so I am adding some awesome sources that I am using to learn about SWIM. (that frees up my time to try to build some cool thing with it)</p><p>The first resource would be the <a href="https://www.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf">SWIM paper</a> itself.</p><p>The second link would be the <a href="https://github.com/hashicorp/memberlist">hashicorp/memberlist</a> library source code that implements SWIM.</p><p>The next is one of the best ways to quickly get a taste for SWIM. It is a “papers we love” video presentation of the SWIM paper by Armon Dadgar from HashiCorp. (linked below) </p><iframe width="560" height="315" src="https://www.youtube.com/embed/bkmbWsDz8LM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p>In the presentation, they mention the missing pieces of SWIM and arrive at improvements to SWIM which makes it production-ready. For example, SWIM implementation by the original paper authors was using 56 nodes, but Hashicorp’s implementation was tested in a production use-case having 2000+ nodes. In those massive number of node environments, we need to deal with rejoins efficiently, encryption, etc.</p><p>They took all these improvements and present a paper named <a href="https://arxiv.org/abs/1707.00788">Lifeguard: Local Health Awareness for More Accurate Failure Detection</a>. Furthermore, they created this project called <a href="https://www.serf.io">Serf</a> which builds on top of the memberlist library that can be tuned to have a raw SWIM implementation. So if we need a production-ready SWIM implementation, we might just use the Serf library.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I finished reading a research paper called “SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol” last nigh
      
    
    </summary>
    
    
      <category term="databases" scheme="https://vishnubharathi.codes/tags/databases/"/>
    
      <category term="research papers" scheme="https://vishnubharathi.codes/tags/research-papers/"/>
    
  </entry>
  
  <entry>
    <title>Expense sheets</title>
    <link href="https://vishnubharathi.codes/blog/expense-sheets/"/>
    <id>https://vishnubharathi.codes/blog/expense-sheets/</id>
    <published>2020-12-28T14:39:26.000Z</published>
    <updated>2021-02-03T02:55:21.204Z</updated>
    
    <content type="html"><![CDATA[<p>As the end of the year approaches, I just realized that I was able to consistently track my expenses with an Excel sheet.</p><p><img src="/images/excel-expenses.png" alt="excel expenses"></p><p>I usually start a month by copy-pasting the “Template” (that contains my mandatory expenses like bills payments, EMIs, etc.) and naming the sheet to the current month’s name. This has helped me to keep track of “things I have to pay” and “things I already paid” the current month.</p><p>I recently discovered <a href="https://julian.digital">Julian’s blog</a> where he <a href="https://julian.digital/?s=media+consumption">tracks media consumption every month</a>. That’s a nice way to build up a collection of quality things over time.</p><p>While I always thought that it is difficult to achieve such a level of consistency, I just surprisingly found out that I had consistently done my expense tracking for more than 2 years now.</p><p>I started this habit when I was trying out <a href="https://www.notion.so/">notion</a> in 2018.</p><p><img src="/images/notion-expenses.png" alt="notion expenses"></p><p>But around August 2019, I hit my max space limit on the notion free tier and I was not able to upgrade to a paid plan at that time. So, I thought I should just use an excel sheet in google drive for tracking this.</p><p>A lesson that I can infer from this activity just while writing this blog post is “we can achieve consistency while trying to be consistent in something that is really useful to us”. The expense tracking was useful to me because without the tracking I might miss paying out bills on time :D So, I think I was able to be consistent with it.</p><p>I am thinking of some modifications to my process</p><ul><li>Track non-mandatory expenses (like a one-time purchase). I haven’t done this yet because having them in my sheet didn’t prove to be helpful when I tried doing them) previously.</li><li>Use one excel workbook for all money-related stuff. I had been using one excel workbook per year but I just figured out that we could do excel formula across different sheets in one workbook. This helps to arrive at useful results like “total expenses for a year” - feels much like how companies move toward monorepo while maintaining source code :D</li><li>Leverage the data more. We will try to squeeze out more information by analyzing the data.</li></ul><p>Personal finance is becoming a topic of interest to me more and more these days. If you track your expenses or have some productivity tips about personal finance, do share them with me :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;As the end of the year approaches, I just realized that I was able to consistently track my expenses with an Excel sheet.&lt;/p&gt;
&lt;p&gt;&lt;img src
      
    
    </summary>
    
    
      <category term="personal finance" scheme="https://vishnubharathi.codes/tags/personal-finance/"/>
    
      <category term="life" scheme="https://vishnubharathi.codes/tags/life/"/>
    
      <category term="productivity" scheme="https://vishnubharathi.codes/tags/productivity/"/>
    
  </entry>
  
  <entry>
    <title>Jio</title>
    <link href="https://vishnubharathi.codes/blog/jio/"/>
    <id>https://vishnubharathi.codes/blog/jio/</id>
    <published>2020-11-01T18:28:00.000Z</published>
    <updated>2021-02-03T02:55:21.204Z</updated>
    
    <content type="html"><![CDATA[<p>This might be a silly and short blog post :D I am writing this to let the world know that I got a new “Jio” SIM card today. I was struggling with poor internet speeds and network problems on Airtel. So, I finally took the plunge to get on the Jio network. I should have done this ages ago.</p><p>I was planning to get a <a href="https://www.jio.com/shop/en-in/router-m2-black/p/491193575">Router M2 Black JioFI</a>, but it is out of stock everywhere. So, just got the SIM card and using via mobile.</p><p>For people who don’t have context about what Jio is: It is a popular mobile network that changed a lot of things in India. I totally love what Jio is doing to India.</p><p>It is one of the first networks to charge only for the mobile data and give calls for free. It is the network that is helping a lot of people in India to get access to the internet. The internet speeds used to crawl and still crawls in many other networks. But Jio is super-fast and very very decent.</p><p>When Jio came out first, I didn’t take it much seriously. I wasn’t even as excited as when Videocon introduced a mobile network. In fact, they seemed sketchy for me just after my experience with Videocon (introduce unbelievable plans and charge high after sometime and eventually cease operation - that’s Videocon’s story in short.)</p><p>Super speed at dirt cheap prices; not many brands give these. Jio does this awesomely and at scale. How do I know? Almost all the people in my family have Jio ( except for me - not true anymore :D ) I have used their mobile networks via wifi hotspots because my network gave problems and everything it solves problems for me.</p><p>Anyways, I felt quiet here and I thought I will just share away this update :D With an upgrade in my internet speed, I just feel like half of my problems are gone :D</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This might be a silly and short blog post :D I am writing this to let the world know that I got a new “Jio” SIM card today. I was struggl
      
    
    </summary>
    
    
      <category term="life" scheme="https://vishnubharathi.codes/tags/life/"/>
    
  </entry>
  
</feed>
